
# EZ-Robot Facial Expression Recognition (Offline Local Model)

This project enables an EZ-Robot (JD Humanoid) to read human facial expressions **without relying on cloud APIs**.  
It streams camera footage from Synthiam ARC to a Python model running locally, classifies facial expressions, and (optionally) sends the emotion back to ARC to trigger robot responses.

---

## ğŸš§ Project Overview

This project was built to solve two main problems:

1. **ARCâ€™s built-in AI vision isn't reliable offline**  
2. We needed a **local, fast, and customizable ML model** to classify emotions from the robotâ€™s camera feed

The solution:

- Use ARC to control the robot + host the camera stream
- Use Python to run an offline ML model for emotion detection
- (Optional) Send the detected emotion back to ARC for robot behaviors

---

## âœ… Requirements

### Hardware
- EZ-Robot JD Humanoid (or any EZ-B v4 based robot with camera)
- Computer on the **same network** as the robot

### Software
| Component | Version | Notes |
|-----------|----------|--------|
| Synthiam ARC | Latest | Must have Camera and EZB skills installed |
| Python | 3.9â€“3.11 | **Do not use 3.12+** due to ML dependency issues |
| pip packages | See `requirements.txt` | Install after cloning repo |

> If you skip matching Python versions, expect dependency failures.

---

## ğŸ“¦ Setup Instructions

### 1. Install Synthiam ARC

Download & install from:  
https://synthiam.com/Software/ARC

Run ARC once after installation so it finishes module setup.

---

### 2. Clone the Repository

```bash
git clone <your_repo_url_here>
cd <repo_folder_name>
```

Replace the placeholder with your actual repo link.

---

### 3. Open the ARC Project

1. Launch Synthiam ARC  
2. Click **Open Project**
3. Select the included `.ezproject` file from this repo

**If you donâ€™t see the project file, you're in the wrong directory.**

---

### 4. Install Python Dependencies

From inside the repo directory:

```bash
pip install -r requirements.txt
```

If errors appear, **read them**. Most issues are missing OS packages or wrong Python version.

---

## ğŸ¤– Robot Connection & Camera Setup

### 5. Connect to the Robot Network

You have two options:

| Mode | When to Use |
|--------|----------------|
| Connect to EZ-B Wi-Fi | Initial setup or direct connection |
| Use Home/Local LAN | For stable twoâ€‘way messaging between ARC & Python |

ARC must show a successful EZ-B connection before proceeding.

---

### 6. Connect to the Robot in ARC

In ARC:

1. Add or open the **EZ-B Connection** skill  
2. Enter the EZ-Bâ€™s IP address  
3. Click **Connect**

You should hear the robot speak or chime when connected.

---

### 7. Start Camera Feed

In ARC:

1. Add the **Camera Device** skill (if not already present)
2. Select the EZ-Robot Camera
3. Click **Start** to confirm live video feed

If the camera fails here, **stopâ€”Python wonâ€™t fix this.** Fix ARC first.

---

### 8. Start Camera Stream Server

Still in ARC Camera Skill:

- Enable **HTTP/MJPEG Camera Streaming**
- Note the stream URL provided (e.g. `http://192.168.1.1:24` or similar)

Test the stream in a browser **first**.  
If your browser canâ€™t load it, Python wonâ€™t either.

Example test URL check:

```
http://<robot_ip>:<port>/
```

---

## ğŸ§  Run Local Emotion Detection Model

### 9. Run the Local Test Script

```bash
python test_model.py
```

The script will:

- Pull frames from the ARC camera stream
- Run the trained model locally
- Print or display the detected emotions in realâ€‘time

If it errors immediately, check:
- Wrong stream URL in script
- Missing model file(s)
- Wrong Python version

---

## ğŸ§ª Troubleshooting Checklist

If something fails, check these **in this order**:

| Step | Must be True |
|------|----------------|
| ARC installed and runs | âœ… |
| Robot connects in ARC | âœ… |
| Camera feed visible in ARC | âœ… |
| Stream opens in browser | âœ… |
| Python dependencies installed | âœ… |
| `test_model.py` runs without crashing | âœ… |

If any âŒ is true, fix that step first. Donâ€™t skip ahead.

---

## ğŸš€ Next Phase: Robot Reactions (Optional)

Once basic detection works, you can expand functionality:

| Feature | Description |
|----------|----------------|
| Send emotion back to ARC | Trigger robot animations, speech, LED responses |
| Replace model with a better one | Increase accuracy, add more emotions |
| Make script headless | Run without UI for performance |
| Bundle into ARC behavior control | Fully integrate into robot behaviors |

If you want, a `robot_actions.py` module can be added to handle reactions automatically.

---

## ğŸ“‚ Expected Project Structure

```
project_root/
â”‚
â”œâ”€ arc_project/
â”‚   â””â”€ your_project.ezproject
â”‚
â”œâ”€ model/
â”‚   â”œâ”€ model.pkl
â”‚   â””â”€ labels.json
â”‚
â”œâ”€ test_model.py
â”œâ”€ requirements.txt
â””â”€ README.md
```

---

## âœ¨ Credits

Developed as part of a hands-on project to enable **offline** emotion recognition for EZâ€‘Robots using Python + ARC integrations.

---

If you'd like, I can also generate a **Phase 2 README** for integrating the robotâ€™s reaction system once we finish the test phase.
